{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter path of an image: 1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train] [--validate] [--decoder {bestpath,beamsearch,wordbeamsearch}]\n",
      "                             [--batch_size BATCH_SIZE] [--data_dir DATA_DIR] [--fast] [--dump]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\shobh\\AppData\\Roaming\\jupyter\\runtime\\kernel-1f51e67b-8dc7-4c1a-bb87-42970ad9e82e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import editdistance\n",
    "from path import Path\n",
    "\n",
    "from DataLoaderIAM import DataLoaderIAM, Batch\n",
    "from Model import Model, DecoderType\n",
    "from SamplePreprocessor import preprocess\n",
    "\n",
    "path = input(\"Enter path of an image: \")\n",
    "path = \"../data/new/\" + path\n",
    "\n",
    "class FilePaths:\n",
    "    \"filenames and paths to data\"\n",
    "    fnCharList = '../model/charList.txt'\n",
    "    fnSummary = '../model/summary.json'\n",
    "    fnInfer = path\n",
    "    fnCorpus = '../data/corpus.txt'\n",
    "\n",
    "\n",
    "def write_summary(charErrorRates, wordAccuracies):\n",
    "    with open(FilePaths.fnSummary, 'w') as f:\n",
    "        json.dump({'charErrorRates': charErrorRates, 'wordAccuracies': wordAccuracies}, f)\n",
    "\n",
    "\n",
    "def train(model, loader):\n",
    "    \"train NN\"\n",
    "    epoch = 0  # number of training epochs since start\n",
    "    summaryCharErrorRates = []\n",
    "    summaryWordAccuracies = []\n",
    "    bestCharErrorRate = float('inf')  # best valdiation character error rate\n",
    "    noImprovementSince = 0  # number of epochs no improvement of character error rate occured\n",
    "    earlyStopping = 25  # stop training after this number of epochs without improvement\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        # train\n",
    "        print('Train NN')\n",
    "        loader.trainSet()\n",
    "        while loader.hasNext():\n",
    "            iterInfo = loader.getIteratorInfo()\n",
    "            batch = loader.getNext()\n",
    "            loss = model.trainBatch(batch)\n",
    "            print(f'Epoch: {epoch} Batch: {iterInfo[0]}/{iterInfo[1]} Loss: {loss}')\n",
    "\n",
    "        # validate\n",
    "        charErrorRate, wordAccuracy = validate(model, loader)\n",
    "\n",
    "        # write summary\n",
    "        summaryCharErrorRates.append(charErrorRate)\n",
    "        summaryWordAccuracies.append(wordAccuracy)\n",
    "        write_summary(summaryCharErrorRates, summaryWordAccuracies)\n",
    "\n",
    "        # if best validation accuracy so far, save model parameters\n",
    "        if charErrorRate < bestCharErrorRate:\n",
    "            print('Character error rate improved, save model')\n",
    "            bestCharErrorRate = charErrorRate\n",
    "            noImprovementSince = 0\n",
    "            model.save()\n",
    "        else:\n",
    "            print(f'Character error rate not improved, best so far: {charErrorRate * 100.0}%')\n",
    "            noImprovementSince += 1\n",
    "\n",
    "        # stop training if no more improvement in the last x epochs\n",
    "        if noImprovementSince >= earlyStopping:\n",
    "            print(f'No more improvement since {earlyStopping} epochs. Training stopped.')\n",
    "            break\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    \"validate NN\"\n",
    "    print('Validate NN')\n",
    "    loader.validationSet()\n",
    "    numCharErr = 0\n",
    "    numCharTotal = 0\n",
    "    numWordOK = 0\n",
    "    numWordTotal = 0\n",
    "    while loader.hasNext():\n",
    "        iterInfo = loader.getIteratorInfo()\n",
    "        print(f'Batch: {iterInfo[0]} / {iterInfo[1]}')\n",
    "        batch = loader.getNext()\n",
    "        (recognized, _) = model.inferBatch(batch)\n",
    "\n",
    "        print('Ground truth -> Recognized')\n",
    "        for i in range(len(recognized)):\n",
    "            numWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\n",
    "            numWordTotal += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gtTexts[i])\n",
    "            numCharErr += dist\n",
    "            numCharTotal += len(batch.gtTexts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gtTexts[i] + '\"', '->',\n",
    "                  '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # print validation result\n",
    "    charErrorRate = numCharErr / numCharTotal\n",
    "    wordAccuracy = numWordOK / numWordTotal\n",
    "    print(f'Character error rate: {charErrorRate * 100.0}%. Word accuracy: {wordAccuracy * 100.0}%.')\n",
    "    return charErrorRate, wordAccuracy\n",
    "\n",
    "\n",
    "def infer(model, fnImg):\n",
    "    \"recognize text in image provided by file path\"\n",
    "    img = preprocess(cv2.imread(fnImg, cv2.IMREAD_GRAYSCALE), Model.imgSize)\n",
    "    batch = Batch(None, [img])\n",
    "    (recognized, probability) = model.inferBatch(batch, True)\n",
    "    print(f'Predicted Text : \" {recognized[0]} \"')\n",
    "    final = 'Predicted Text : '\n",
    "    final = final + recognized[0]\n",
    "    return final\n",
    "    #print(f'Probability: {probability[0]}')\n",
    "    #img1 = cv2.imread(path)\n",
    "    #img1 = cv2.resize(img1, (500, 250))\n",
    "    #cv2.imshow(final,img1)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"main function\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', help='train the NN', action='store_true')\n",
    "    parser.add_argument('--validate', help='validate the NN', action='store_true')\n",
    "    parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath',\n",
    "                        help='CTC decoder')\n",
    "    parser.add_argument('--batch_size', help='batch size', type=int, default=100)\n",
    "    parser.add_argument('--data_dir', help='directory containing IAM dataset', type=Path, required=False)\n",
    "    parser.add_argument('--fast', help='use lmdb to load images', action='store_true')\n",
    "    parser.add_argument('--dump', help='dump output of NN to CSV file(s)', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # set chosen CTC decoder\n",
    "    if args.decoder == 'bestpath':\n",
    "        decoderType = DecoderType.BestPath\n",
    "    elif args.decoder == 'beamsearch':\n",
    "        decoderType = DecoderType.BeamSearch\n",
    "    elif args.decoder == 'wordbeamsearch':\n",
    "        decoderType = DecoderType.WordBeamSearch\n",
    "\n",
    "    # train or validate on IAM dataset\n",
    "    if args.train or args.validate:\n",
    "        # load training data, create TF model\n",
    "        loader = DataLoaderIAM(args.data_dir, args.batch_size, Model.imgSize, Model.maxTextLen, args.fast)\n",
    "\n",
    "        # save characters of model for inference mode\n",
    "        open(FilePaths.fnCharList, 'w').write(str().join(loader.charList))\n",
    "\n",
    "        # save words contained in dataset into file\n",
    "        open(FilePaths.fnCorpus, 'w').write(str(' ').join(loader.trainWords + loader.validationWords))\n",
    "\n",
    "        # execute training or validation\n",
    "        if args.train:\n",
    "            model = Model(loader.charList, decoderType)\n",
    "            train(model, loader)\n",
    "        elif args.validate:\n",
    "            model = Model(loader.charList, decoderType, mustRestore=True)\n",
    "            validate(model, loader)\n",
    "\n",
    "    # infer text on test image\n",
    "    else:\n",
    "        model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)\n",
    "        m = infer(model, FilePaths.fnInfer)\n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = main()\n",
    "    print(\"Hello\",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "\n",
    "# define a folder to store and later serve the images\n",
    "UPLOAD_FOLDER = '/data/new/'\n",
    "\n",
    "# allow files of a specific type\n",
    "ALLOWED_EXTENSIONS = set(['png', 'jpg', 'jpeg','jfif'])\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# function to check the file extension\n",
    "def allowed_file(filename):  \n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "# route and function to handle the home page\n",
    "@app.route('/')\n",
    "def home_page():  \n",
    "    return render_template('index.html')\n",
    "\n",
    "# route and function to handle the upload page\n",
    "@app.route('/upload', methods=['GET', 'POST'])\n",
    "def upload_page():\n",
    "    global disease\n",
    "    disease = ''\n",
    "    if request.method == 'POST':\n",
    "        # check if there is a file in the request\n",
    "        if 'file' not in request.files:\n",
    "            return render_template('upload.html', msg='No file selected')\n",
    "        file = request.files['file']\n",
    "        # if no file is selected\n",
    "        if file.filename == '':\n",
    "            return render_template('upload.html', msg='No file selected')\n",
    "        \n",
    "        file.save(os.path.join(UPLOAD_FOLDER, file.filename))\n",
    "\n",
    "        if file and allowed_file(file.filename):\n",
    "\n",
    "            \n",
    "            img_src=UPLOAD_FOLDER + file.filename\n",
    "            disease = main(img_src)\n",
    "\n",
    "           \n",
    "            return render_template('upload.html',\n",
    "                                   msg='Successfully processed:\\n'+file.filename,\n",
    "                                   disease=disease,\n",
    "                                   img_src=UPLOAD_FOLDER + file.filename)\n",
    "    elif request.method == 'GET':\n",
    "        return render_template('upload.html')\n",
    "if __name__ == '__main__':  \n",
    "    app.run(debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
